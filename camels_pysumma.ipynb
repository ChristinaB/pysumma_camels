{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the SUMMA setup\n",
    "To begin, we have to regionalize the paths in the configuration files that SUMMA will use.\n",
    "This is accomplished by running a shell command. This is done by starting a line with the `!` operator.\n",
    "We simply run a script to complete the installation.\n",
    "Then, we can import some basic libraries along with `pysumma`.\n",
    "The `%pylab inline` magic command simply imports some standard scientific packages such as `numpy` and `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = '/glade/work/ashleyvb'\n",
    "folder = top+'/CAMELs'\n",
    "folders = folder+'/summa_camels'\n",
    "! cd /glade/work/ashleyvb/CAMELs/summa_camels; ./install_local_setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we check that we loaded the correct environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /glade/work/ashleyvb/miniconda3/envs/pysumma:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "pysumma                   2.0.0                     dev_0    <develop>\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda list pysumma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Then we load the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pysumma as ps\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCORES=48\n",
    "cluster = PBSCluster(n_workers = NCORES,\n",
    "                     cores=NCORES,\n",
    "                     processes=NCORES, \n",
    "                     memory=\"24GB\",\n",
    "                     project='UWAS0091',\n",
    "                     queue='regular',\n",
    "                     walltime='06:00:00')\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://10.148.2.107:33971' processes=48 threads=48, memory=24.00 GB>\n",
      "Job id            Name             User              Time Use S Queue\n",
      "----------------  ---------------- ----------------  -------- - -----\n",
      "2424017.chadmin1  Jupyter          ashleyvb          00:00:09 R regular         \n",
      "2424079.chadmin1  dask-worker      ashleyvb          00:01:39 R regular         \n"
     ]
    }
   ],
   "source": [
    "# Check that have workers, do not run the rest of the cells until the workers show up. \n",
    "print(client)\n",
    "!qstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <br>\n",
    "\n",
    "# Interacting with SUMMA via the `Distributed` object\n",
    "\n",
    "We are running a `Distributed` object, which has multiple `Simulation` objects inside, each corresponding to some spatial chunk. \n",
    "We need to do `rm -r /glade/work/ashleyvb/CAMELs/summa_camels/.pysumma/` to clear out the distributed folders every run so permissions do not get screwed up in the loops. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Instantiating a `Distributed` object\n",
    "\n",
    "To set up a `Distributed` object you must supply several pieces of information. \n",
    "First, supply the SUMMA executable; this could be either the compiled executable on your local machine, or a docker image. \n",
    "The second piece of information is the path to the file manager, which we just created through the install script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable = top+'/summa/bin/summa.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_manager = folders+'/file_manager.txt'\n",
    "camels = ps.Distributed(executable, file_manager, num_workers=NCORES, chunk_size=8, client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Manipulating the configuration of the distributed object\n",
    "\n",
    "Currrently, none of the attributes that can be changed in a SUMMA `Simulation` object cannot be altered in a `Distributed` object and the only one that can be viewed is the file manager. In the notebook that made the forcing files, we wrote file managers. \n",
    "To see a file manager, simply `print` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'SUMMA_FILE_MANAGER_V1.0'    ! filemanager_version\n",
      "'/glade/work/ashleyvb/CAMELs/summa_camels/settings.v1/'    ! settings_path\n",
      "'/glade/work/ashleyvb/CAMELs/summa_camels/forcing/1hr/'    ! input_path\n",
      "'/glade/work/ashleyvb/CAMELs/summa_camels/output/1hr/'    ! output_path\n",
      "'modelDecisions.1hr.txt'    ! decisions_path\n",
      "'[notUsed]'    ! meta_time\n",
      "'[notUsed]'    ! meta_attr\n",
      "'[notUsed]'    ! meta_type\n",
      "'[notUsed]'    ! meta_force\n",
      "'[notUsed]'    ! meta_localparam\n",
      "'output_control2.txt'    ! output_control\n",
      "'[notUsed]'    ! meta_localindex\n",
      "'[notUsed]'    ! meta_basinparam\n",
      "'[notUsed]'    ! meta_basinmvar\n",
      "'../settings.v1/attributes.camels.v2.nc'    ! local_attributes\n",
      "'../settings.v1/localParamInfo.txt'    ! local_param_info\n",
      "'../settings.v1/basinParamInfo.txt'    ! basin_param_info\n",
      "'../settings.v1/forcingFileList.1hr.txt'    ! forcing_file_list\n",
      "'../settings.v1/coldState.8lyr.nc'    ! model_init_cond\n",
      "'../settings.v1/trialParams.camels.v1.nc'    ! parameter_trial\n",
      "'camels_1hr_'    ! output_prefix\n"
     ]
    }
   ],
   "source": [
    "print(camels.manager) #possible days 1980-01-01 to 2018-12-31, we are running 1986-10-01 01:00 to 1991-10-02 0:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Running pySUMMA for all Forcing Files\n",
    "\n",
    "We now run all 671 basins of pySUMMA for each set of forcing files. You can check how long it has been running by using the command `qstat -u <username>` in a terminal. Each run takes about 9 minutes. First, we start with the original NLDAS files, or the \"truth run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.8 s, sys: 16.8 s, total: 1min 5s\n",
      "Wall time: 9min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "camels.run('local')\n",
    "#all_status = [(n, s.status) for n, s in camels.simulations.items()] #if want to look at status if has errors\n",
    "all_ds = [s.output.load() for n, s in camels.simulations.items()] #load it into memory so faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could just write it as several files instead of merging. However, if we want to merge, we can do the following.\n",
    "First, detect automatically which vars have hru vs gru dimensions (depending on what we use for output, we may not have any gru):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hru_vars = [] # variables that have hru dimension\n",
    "gru_vars = [] # variables that have gru dimension\n",
    "for ds in all_ds:\n",
    "    for name, var in ds.variables.items():\n",
    "        if 'hru' in var.dims:\n",
    "            hru_vars.append(name)\n",
    "        elif 'gru' in var.dims:\n",
    "            gru_vars.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter variables for merge, this takes seconds since we are running a limiited output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.21 s, sys: 1.75 s, total: 9.95 s\n",
      "Wall time: 9.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hru_ds = [ds[hru_vars] for ds in all_ds]\n",
    "gru_ds = [ds[gru_vars] for ds in all_ds]\n",
    "hru_merged = xr.concat(hru_ds, dim='hru')\n",
    "gru_merged = xr.concat(gru_ds, dim='gru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:                (hru: 671, time: 43848)\n",
      "Coordinates:\n",
      "  * time                   (time) datetime64[ns] 1986-10-01T00:59:59.999986592 ... 1991-10-02\n",
      "  * hru                    (hru) int64 1 2 3 4 5 6 7 ... 666 667 668 669 670 671\n",
      "Data variables:\n",
      "    pptrate                (time, hru) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
      "    airtemp                (time, hru) float64 290.6 293.8 292.6 ... 303.2 300.3\n",
      "    spechum                (time, hru) float64 0.01139 0.0129 ... 0.006427\n",
      "    windspd                (time, hru) float64 4.275 4.809 4.036 ... 6.271 6.169\n",
      "    SWRadAtm               (time, hru) float64 0.0 0.0 0.0 ... 294.0 300.4 295.8\n",
      "    LWRadAtm               (time, hru) float64 377.2 400.1 381.0 ... 329.1 319.0\n",
      "    airpres                (time, hru) float64 9.743e+04 9.944e+04 ... 9.412e+04\n",
      "    scalarCanopyWat        (time, hru) float64 0.0 0.0 0.0 ... 0.003957 0.003082\n",
      "    scalarSWE              (time, hru) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
      "    scalarTotalSoilWat     (time, hru) float64 1.496e+03 945.0 ... 861.9 952.9\n",
      "    scalarSenHeatTotal     (time, hru) float64 104.4 141.7 ... -45.47 -3.31\n",
      "    scalarLatHeatTotal     (time, hru) float64 -0.1574 -0.09865 ... -12.34 -61.1\n",
      "    scalarSnowSublimation  (time, hru) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
      "    scalarRainPlusMelt     (time, hru) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
      "    scalarInfiltration     (time, hru) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
      "    scalarSurfaceRunoff    (time, hru) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
      "    scalarSoilDrainage     (time, hru) float64 1.954e-07 1.654e-08 ... 7.618e-09\n",
      "    scalarAquiferBaseflow  (time, hru) float64 0.0002729 0.0002767 ... 7.623e-09\n",
      "    scalarTotalET          (time, hru) float64 -6.293e-08 ... -2.443e-05\n",
      "    scalarTotalRunoff      (time, hru) float64 0.0002729 0.0002767 ... 7.623e-09\n",
      "    scalarNetRadiation     (time, hru) float64 -19.22 -11.75 ... 80.46 82.19\n",
      "    hruId                  (hru) int64 1013500 1022500 ... 14362250 14400000\n",
      "Attributes:\n",
      "    summaVersion:     v2.0.0\n",
      "    buildTime:        Mon May  4 07:07:10 MDT 2020\n",
      "    gitBranch:        develop-0-g942c156\n",
      "    gitHash:          942c15653da097ae1b90bc3093f02757cbf78367\n",
      "    tmZoneInfo:       utcTime\n",
      "    soilStress:       NoahType\n",
      "    stomResist:       BallBerry\n",
      "    num_method:       itertive\n",
      "    fDerivMeth:       analytic\n",
      "    LAI_method:       monTable\n",
      "    notPopulatedYet:  notPopulatedYet\n",
      "    f_Richards:       mixdform\n",
      "    groundwatr:       bigBuckt\n",
      "    hc_profile:       constant\n",
      "    bcUpprTdyn:       nrg_flux\n",
      "    bcLowrTdyn:       zeroFlux\n",
      "    bcUpprSoiH:       liq_flux\n",
      "    bcLowrSoiH:       drainage\n",
      "    veg_traits:       Raupach_BLM1994\n",
      "    canopyEmis:       difTrans\n",
      "    snowIncept:       lightSnow\n",
      "    windPrfile:       logBelowCanopy\n",
      "    astability:       louisinv\n",
      "    canopySrad:       BeersLaw\n",
      "    alb_method:       conDecay\n",
      "    snowLayers:       CLM_2010\n",
      "    compaction:       anderson\n",
      "    thCondSnow:       jrdn1991\n",
      "    thCondSoil:       funcSoilWet\n",
      "    spatial_gw:       localColumn\n",
      "    subRouting:       timeDlay\n"
     ]
    }
   ],
   "source": [
    "print(hru_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.22 s, sys: 6.31 s, total: 15.5 s\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hru_merged.to_netcdf(folders+'/output/merged_day/NLDAS_1hr_hru.nc')\n",
    "gru_merged.to_netcdf(folders+'/output/merged_day/NLDAS_1hr_gru.nc')\n",
    "del camels\n",
    "del all_ds \n",
    "del hru_merged\n",
    "del gru_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Here are the other runs, now as a loop. The processes are the same, but for clarity we will divide it into 2 loops, one for the constant forcings and one for the MetSim forcings. This will take about an hour for each loop. We delete stuff after every run to reduce memory needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set forcings to hold at constant or MetSim and create dictionaries\n",
    "constant_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd','all']\n",
    "metsim_vars  = ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd','all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airpres\n",
      "CPU times: user 1min 7s, sys: 21.5 s, total: 1min 29s\n",
      "Wall time: 8min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Constant\n",
    "for v in constant_vars:\n",
    "    ! rm -rf /glade/work/ashleyvb/CAMELs/summa_camels/.pysumma \n",
    "    file_manager = folders+'/file_manager_constant_' + v +'.txt'\n",
    "    camels = ps.Distributed(executable, file_manager, num_workers=NCORES, chunk_size=8, client=client)   \n",
    "    camels.run('local')\n",
    "    #all_status = [(n, s.status) for n, s in camels.simulations.items()] #if want to look at status if has errors\n",
    "    all_ds = [s.output.load() for n, s in camels.simulations.items()] #load it into memory so faster    \n",
    "    hru_vars = [] # variables that have hru dimension\n",
    "    gru_vars = [] # variables that have gru dimension\n",
    "    for ds in all_ds:\n",
    "        for name, var in ds.variables.items():\n",
    "            if 'hru' in var.dims:\n",
    "                hru_vars.append(name)\n",
    "            elif 'gru' in var.dims:\n",
    "                gru_vars.append(name)\n",
    "    hru_ds = [ds[hru_vars] for ds in all_ds]\n",
    "    gru_ds = [ds[gru_vars] for ds in all_ds]\n",
    "    hru_merged = xr.concat(hru_ds, dim='hru')\n",
    "    gru_merged = xr.concat(gru_ds, dim='gru')\n",
    "    hru_merged.to_netcdf(folders+'/output/merged_day/NLDASconstant_' + v +'_hru.nc')\n",
    "    gru_merged.to_netcdf(folders+'/output/merged_day/NLDASconstant_' + v +'_gru.nc')\n",
    "    del camels\n",
    "    del all_ds \n",
    "    del hru_merged\n",
    "    del gru_merged\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airpres\n",
      "airtemp\n",
      "LWRadAtm\n",
      "pptrate\n",
      "spechum\n",
      "SWRadAtm\n",
      "windspd\n",
      "all\n",
      "CPU times: user 8min 56s, sys: 2min 54s, total: 11min 51s\n",
      "Wall time: 1h 9min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Metsim\n",
    "for v in metsim_vars:\n",
    "    ! rm -rf /glade/work/ashleyvb/CAMELs/summa_camels/.pysumma \n",
    "    file_manager = folders+'/file_manager_metsim_' + v +'.txt'\n",
    "    camels = ps.Distributed(executable, file_manager, num_workers=NCORES, chunk_size=8, client=client)   \n",
    "    camels.run('local')\n",
    "    #all_status = [(n, s.status) for n, s in camels.simulations.items()] #if want to look at status if has errors\n",
    "    all_ds = [s.output.load() for n, s in camels.simulations.items()] #load it into memory so faster    \n",
    "    hru_vars = [] # variables that have hru dimension\n",
    "    gru_vars = [] # variables that have gru dimension\n",
    "    for ds in all_ds:\n",
    "        for name, var in ds.variables.items():\n",
    "            if 'hru' in var.dims:\n",
    "                hru_vars.append(name)\n",
    "            elif 'gru' in var.dims:\n",
    "                gru_vars.append(name)\n",
    "    hru_ds = [ds[hru_vars] for ds in all_ds]\n",
    "    gru_ds = [ds[gru_vars] for ds in all_ds]\n",
    "    hru_merged = xr.concat(hru_ds, dim='hru')\n",
    "    gru_merged = xr.concat(gru_ds, dim='gru')\n",
    "    hru_merged.to_netcdf(folders+'/output/merged_day/NLDASmetsim_' + v +'_hru.nc')\n",
    "    gru_merged.to_netcdf(folders+'/output/merged_day/NLDASmetsim_' + v +'_gru.nc')\n",
    "    del camels\n",
    "    del all_ds \n",
    "    del hru_merged\n",
    "    del gru_merged\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Here is the code to run a smaller test set that is not in distributed mode if you have errors and want to see them.\n",
    "s = ps.Simulation(executable, file_manager)\n",
    "s.decisions['simulStart'] = '1980-04-02 00:00'\n",
    "s.decisions['simulFinsh'] = '1980-04-02 23:00'\n",
    "print(s.decisions)\n",
    "s.run('local', run_suffix='_default')\n",
    "assert s.status == 'Success'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(s.stderr)\n",
    "print(s.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysumma",
   "language": "python",
   "name": "pysumma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
